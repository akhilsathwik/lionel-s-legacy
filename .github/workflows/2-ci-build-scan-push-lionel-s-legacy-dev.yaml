name: "CI/CD: Dev Environment - lionel-s-legacy"

concurrency:
  group: ci-lionel-s-legacy-dev-${{ github.ref }}
  cancel-in-progress: false

on:
  push:
    branches: [main]
    paths-ignore:
      - '.opsera-**/k8s/**'
      - '.github/workflows/**'
      - '.opsera-**/**'
      - '.deployments/**'
      - '**/*.md'
      - '**/*.disabled'
  workflow_dispatch:
    inputs:
      environment:
        description: 'Environment to deploy'
        required: true
        default: 'dev'
        type: choice
        options: ['dev']

permissions:
  contents: write
  id-token: write
  security-events: write

env:
  APP_NAME: lionel-s-legacy
  TENANT: opsera
  ENV: dev
  AWS_REGION: us-west-2
  REGION_SHORT: usw2
  HUB_CLUSTER: argocd-usw2
  SPOKE_CLUSTER: opsera-usw2-np
  ECR_REPO: opsera/lionel-s-legacy

jobs:
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 1: Check Bootstrap Prerequisites
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  check-bootstrap-prerequisites:
    name: "ğŸ” Check Bootstrap Prerequisites"
    runs-on: ubuntu-latest
    outputs:
      bootstrap_complete: ${{ steps.check.outputs.bootstrap_complete }}
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Check Prerequisites
        id: check
        run: |
          FAILED=0

          # Check ECR repository
          echo "ğŸ” Checking ECR repository..."
          if aws ecr describe-repositories --repository-names "${{ env.ECR_REPO }}" &>/dev/null; then
            echo "âœ… ECR repository exists"
          else
            echo "âŒ ECR repository not found: ${{ env.ECR_REPO }}"
            FAILED=1
          fi

          # Check EKS clusters
          echo "ğŸ” Checking EKS clusters..."
          aws eks update-kubeconfig --name ${{ env.HUB_CLUSTER }} --region ${{ env.AWS_REGION }} --alias hub
          aws eks update-kubeconfig --name ${{ env.SPOKE_CLUSTER }} --region ${{ env.AWS_REGION }} --alias spoke

          # Check namespace on spoke
          NS="${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENV }}"
          if kubectl --context spoke get namespace "$NS" &>/dev/null; then
            echo "âœ… Namespace exists: $NS"
          else
            echo "âŒ Namespace not found: $NS"
            FAILED=1
          fi

          # Check ArgoCD repo registration on hub
          REPO_SECRET="repo-${{ github.repository_owner }}-${{ env.APP_NAME }}"
          if kubectl --context hub get secret "$REPO_SECRET" -n argocd &>/dev/null; then
            echo "âœ… Repo registered with ArgoCD"
          else
            echo "âš ï¸ Repo not registered with ArgoCD (will be handled)"
          fi

          # Check spoke registration on hub
          CLUSTER_SECRET="cluster-${{ env.SPOKE_CLUSTER }}"
          if kubectl --context hub get secret "$CLUSTER_SECRET" -n argocd &>/dev/null; then
            echo "âœ… Spoke cluster registered"
          else
            echo "âŒ Spoke cluster not registered"
            FAILED=1
          fi

          if [ "$FAILED" -eq 1 ]; then
            echo "âŒ Bootstrap prerequisites not met. Run bootstrap first:"
            echo "gh workflow run 0-bootstrap-lionel-s-legacy.yaml -f confirm=yes"
            echo "bootstrap_complete=false" >> $GITHUB_OUTPUT
            exit 1
          fi

          echo "âœ… All bootstrap prerequisites met"
          echo "bootstrap_complete=true" >> $GITHUB_OUTPUT

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 2: Security Scan (Gitleaks)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  security-scan:
    name: "ğŸ”’ Security Scan (Gitleaks)"
    runs-on: ubuntu-latest
    needs: [check-bootstrap-prerequisites]
    if: needs.check-bootstrap-prerequisites.outputs.bootstrap_complete == 'true'
    continue-on-error: ${{ vars.GITLEAKS_MODE == 'warn' }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Gitleaks
        uses: gitleaks/gitleaks-action@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Display Scan Mode
        if: always()
        run: |
          if [ "${{ vars.GITLEAKS_MODE }}" = "warn" ]; then
            echo "âš ï¸ WARN MODE: Secrets detected â†’ Report only, build continues"
          else
            echo "ğŸš« BLOCK MODE: Workflow failed due to secrets"
          fi

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 3: Build Docker Image
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  build-image:
    name: "ğŸ—ï¸ Build Docker Image"
    runs-on: ubuntu-latest
    needs: [security-scan]
    if: always() && (needs.security-scan.result == 'success' || needs.security-scan.result == 'skipped')
    outputs:
      image_tag: ${{ steps.build.outputs.image_tag }}
      ecr_registry: ${{ steps.build.outputs.ecr_registry }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Generate Image Tag
        id: build
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com"
          IMAGE_TAG="${{ env.ENV }}-${GITHUB_SHA::8}-$(date +%Y%m%d%H%M%S)"

          echo "image_tag=${IMAGE_TAG}" >> $GITHUB_OUTPUT
          echo "ecr_registry=${ECR_REGISTRY}" >> $GITHUB_OUTPUT
          echo "ğŸ“¦ Image tag: ${IMAGE_TAG}"

      - name: Build Docker Image
        run: |
          docker build \
            --platform linux/amd64 \
            --tag ${{ env.APP_NAME }}:${{ steps.build.outputs.image_tag }} \
            --file Dockerfile \
            .
          echo "âœ… Docker image built: ${{ env.APP_NAME }}:${{ steps.build.outputs.image_tag }}"

      - name: Save Image for Transfer
        run: |
          docker save ${{ env.APP_NAME }}:${{ steps.build.outputs.image_tag }} -o /tmp/image.tar
          echo "âœ… Image saved as artifact"

      - name: Upload Image Artifact
        uses: actions/upload-artifact@v4
        with:
          name: docker-image
          path: /tmp/image.tar
          retention-days: 1

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 4: Grype Vulnerability Scan
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  grype-scan:
    name: "ğŸ” Grype Vulnerability Scan"
    runs-on: ubuntu-latest
    needs: [build-image]
    if: always() && needs.build-image.result == 'success'
    continue-on-error: ${{ vars.GRYPE_MODE == 'warn' }}
    outputs:
      scan_passed: ${{ steps.scan.outputs.scan_passed }}
    steps:
      - name: Download Image Artifact
        uses: actions/download-artifact@v4
        with:
          name: docker-image
          path: /tmp

      - name: Load Docker Image
        run: |
          docker load -i /tmp/image.tar
          echo "âœ… Image loaded for scanning"

      - name: Run Grype Scan
        id: scan
        uses: anchore/scan-action@v3
        with:
          image: "${{ env.APP_NAME }}:${{ needs.build-image.outputs.image_tag }}"
          severity-cutoff: high
          fail-build: true
          output-format: sarif

      - name: Upload SARIF to GitHub Security
        if: always()
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: ${{ steps.scan.outputs.sarif }}

      - name: Set Scan Output
        if: always()
        run: |
          if [ "${{ steps.scan.outcome }}" = "success" ]; then
            echo "scan_passed=true" >> $GITHUB_OUTPUT
          else
            echo "scan_passed=false" >> $GITHUB_OUTPUT
            if [ "${{ vars.GRYPE_MODE }}" = "warn" ]; then
              echo "âš ï¸ WARN MODE: Vulnerabilities found â†’ Report only, build continues"
            else
              echo "ğŸš« BLOCK MODE: Build blocked due to vulnerabilities"
            fi
          fi

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 5: Push to ECR
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  push-to-ecr:
    name: "ğŸ“¤ Push to ECR"
    runs-on: ubuntu-latest
    needs: [build-image, grype-scan]
    if: always() && needs.build-image.result == 'success' && (needs.grype-scan.outputs.scan_passed == 'true' || vars.GRYPE_MODE == 'warn')
    outputs:
      image_tag: ${{ needs.build-image.outputs.image_tag }}
      full_image_uri: ${{ steps.push.outputs.full_image_uri }}
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        uses: aws-actions/amazon-ecr-login@v2

      - name: Download Image Artifact
        uses: actions/download-artifact@v4
        with:
          name: docker-image
          path: /tmp

      - name: Load and Push Image
        id: push
        run: |
          docker load -i /tmp/image.tar

          ECR_REGISTRY="${{ needs.build-image.outputs.ecr_registry }}"
          IMAGE_TAG="${{ needs.build-image.outputs.image_tag }}"
          FULL_URI="${ECR_REGISTRY}/${{ env.ECR_REPO }}:${IMAGE_TAG}"

          docker tag ${{ env.APP_NAME }}:${IMAGE_TAG} ${FULL_URI}
          docker push ${FULL_URI}

          echo "full_image_uri=${FULL_URI}" >> $GITHUB_OUTPUT
          echo "âœ… Image pushed: ${FULL_URI}"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 6: Update Manifests
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  update-manifests:
    name: "ğŸ“ Update Manifests"
    runs-on: ubuntu-latest
    needs: [push-to-ecr]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GH_PAT }}
          fetch-depth: 0

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Update Kustomize Overlay
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"

          # RULE 196: Pull FIRST before modifying files
          git pull --rebase origin ${{ github.ref_name }}

          KUSTOMIZATION=".${{ env.TENANT }}-${{ env.APP_NAME }}/k8s/overlays/${{ env.ENV }}/kustomization.yaml"
          IMAGE_TAG="${{ needs.push-to-ecr.outputs.image_tag }}"
          ECR_REGISTRY="${{ needs.push-to-ecr.outputs.full_image_uri }}"

          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
          ECR_URI="${AWS_ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com/${{ env.ECR_REPO }}"

          echo "ğŸ“ Updating manifest: $KUSTOMIZATION"
          echo "   newName: ${ECR_URI}"
          echo "   newTag: ${IMAGE_TAG}"

          # RULE 203: Update using sed (always available on GHA runners)
          sed -i "s|newName:.*|newName: ${ECR_URI}|g" "$KUSTOMIZATION"
          sed -i "s|newTag:.*|newTag: ${IMAGE_TAG}|g" "$KUSTOMIZATION"

          echo "Updated kustomization.yaml:"
          cat "$KUSTOMIZATION"

          git add "$KUSTOMIZATION"

          # Check if there are changes to commit
          if git diff --cached --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          git commit -m "chore(deploy): update dev image to ${IMAGE_TAG} [skip ci]"

          # RULE 202: Push with retry loop
          for i in 1 2 3; do
            echo "Push attempt $i/3..."
            if git push origin ${{ github.ref_name }}; then
              echo "âœ… Manifest updated and pushed successfully"
              break
            fi
            echo "âš ï¸ Push failed, pulling latest and retrying..."
            git pull --rebase origin ${{ github.ref_name }}
            sleep 2
            if [ $i -eq 3 ]; then
              echo "âŒ Failed to push after 3 attempts"
              exit 1
            fi
          done

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 7: Create/Update ArgoCD Application (HUB)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  create-argocd-app:
    name: "ğŸ¯ Create/Update ArgoCD App (HUB)"
    runs-on: ubuntu-latest
    needs: [update-manifests]
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          ref: ${{ github.ref_name }}

      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup kubectl - HUB
        run: |
          aws eks update-kubeconfig --name ${{ env.HUB_CLUSTER }} --region ${{ env.AWS_REGION }} --alias hub

      - name: Apply ArgoCD Application
        run: |
          APP_FOLDER=".${{ env.TENANT }}-${{ env.APP_NAME }}"

          # Apply ArgoCD application manifest (idempotent)
          kubectl --context hub apply -f ${APP_FOLDER}/argocd/${{ env.ENV }}/application.yaml

          echo "âœ… ArgoCD application applied"
          kubectl --context hub get application ${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENV }} -n argocd -o wide

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 8: Refresh ECR Secret (SPOKE)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  refresh-ecr-secret:
    name: "ğŸ”‘ Refresh ECR Secret (SPOKE)"
    runs-on: ubuntu-latest
    needs: [create-argocd-app]
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup kubectl - SPOKE
        run: |
          aws eks update-kubeconfig --name ${{ env.SPOKE_CLUSTER }} --region ${{ env.AWS_REGION }} --alias spoke

      - name: Refresh ECR Pull Secret
        run: |
          NS="${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENV }}"
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

          # Wait for namespace
          for i in $(seq 1 10); do
            if kubectl --context spoke get namespace "$NS" &>/dev/null; then
              echo "âœ… Namespace exists: $NS"
              break
            fi
            echo "â³ Waiting for namespace... ($i/10)"
            sleep 5
          done

          # RULE 197: Store ECR password in variable (NOT --docker-password-stdin)
          ECR_PASS=$(aws ecr get-login-password --region ${{ env.AWS_REGION }})

          # RULE 187: Dry-run + apply for idempotent secret creation
          kubectl create secret docker-registry ecr-pull-secret \
            --namespace="$NS" \
            --docker-server="${AWS_ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com" \
            --docker-username=AWS \
            --docker-password="$ECR_PASS" \
            --dry-run=client -o yaml | kubectl --context spoke apply -f -

          echo "âœ… ECR secret refreshed (valid for 12 hours)"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 9: Sync and Deploy (HUB)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  sync-and-deploy:
    name: "ğŸš€ Sync and Deploy (HUB)"
    runs-on: ubuntu-latest
    needs: [refresh-ecr-secret]
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup kubectl - HUB
        run: |
          aws eks update-kubeconfig --name ${{ env.HUB_CLUSTER }} --region ${{ env.AWS_REGION }} --alias hub

      - name: Hard Refresh ArgoCD
        run: |
          APP_NAME="${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENV }}"

          echo "ğŸ”„ Triggering hard refresh..."
          kubectl --context hub annotate application "$APP_NAME" -n argocd \
            argocd.argoproj.io/refresh=hard --overwrite

          sleep 5
          echo "âœ… Hard refresh triggered"

      - name: Trigger Sync
        run: |
          APP_NAME="${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENV }}"

          echo "ğŸš€ Triggering sync..."
          kubectl --context hub patch application "$APP_NAME" -n argocd \
            --type merge -p '{"operation":{"sync":{"prune":true,"syncOptions":["Replace=true"]}}}'

          echo "âœ… Sync triggered"

      - name: Wait for Sync Completion
        run: |
          APP_NAME="${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENV }}"
          MAX_ATTEMPTS=60
          ATTEMPT=0

          echo "â³ Waiting for sync completion (max 5 minutes)..."
          while [ $ATTEMPT -lt $MAX_ATTEMPTS ]; do
            SYNC_STATUS=$(kubectl --context hub get application "$APP_NAME" -n argocd \
              -o jsonpath='{.status.sync.status}' 2>/dev/null || echo "Unknown")
            HEALTH_STATUS=$(kubectl --context hub get application "$APP_NAME" -n argocd \
              -o jsonpath='{.status.health.status}' 2>/dev/null || echo "Unknown")

            echo "  Attempt $((ATTEMPT+1))/$MAX_ATTEMPTS: Sync=$SYNC_STATUS Health=$HEALTH_STATUS"

            if [ "$SYNC_STATUS" = "Synced" ] && [ "$HEALTH_STATUS" = "Healthy" ]; then
              echo "âœ… Application synced and healthy!"
              exit 0
            fi

            if [ "$HEALTH_STATUS" = "Degraded" ]; then
              echo "âš ï¸ Health degraded, waiting for recovery..."
            fi

            ATTEMPT=$((ATTEMPT + 1))
            sleep 5
          done

          echo "âŒ Sync did not complete within timeout"
          kubectl --context hub get application "$APP_NAME" -n argocd -o yaml
          exit 1

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 10: Verify Deployment (SPOKE)
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  verify-deployment:
    name: "âœ… Verify Deployment (SPOKE)"
    runs-on: ubuntu-latest
    needs: [sync-and-deploy, push-to-ecr]
    steps:
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup kubectl - SPOKE
        run: |
          aws eks update-kubeconfig --name ${{ env.SPOKE_CLUSTER }} --region ${{ env.AWS_REGION }} --alias spoke

      - name: Verify Pods
        run: |
          NS="${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENV }}"

          echo "ğŸ” Checking pods in namespace: $NS"
          kubectl --context spoke get pods -n "$NS" -o wide

          # Wait for rollout
          DEGRADED_COUNT=0
          MAX_DEGRADED=6

          for i in $(seq 1 30); do
            READY=$(kubectl --context spoke get deployment ${{ env.APP_NAME }} -n "$NS" \
              -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
            DESIRED=$(kubectl --context spoke get deployment ${{ env.APP_NAME }} -n "$NS" \
              -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "2")

            echo "  Check $i/30: Ready=$READY/$DESIRED"

            if [ "$READY" = "$DESIRED" ] && [ "$READY" != "0" ]; then
              echo "âœ… All pods ready: $READY/$DESIRED"
              break
            fi

            if [ $i -eq 30 ]; then
              echo "âŒ Pods not ready after timeout"
              kubectl --context spoke describe deployment ${{ env.APP_NAME }} -n "$NS"
              kubectl --context spoke get events -n "$NS" --sort-by='.lastTimestamp' | tail -20
              exit 1
            fi

            sleep 10
          done

      - name: Health Check via Port-Forward
        run: |
          NS="${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENV }}"

          # Get first running pod
          POD=$(kubectl --context spoke get pods -n "$NS" -l app=${{ env.APP_NAME }} \
            -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD" ]; then
            echo "âš ï¸ No running pods found for health check"
            exit 0
          fi

          echo "ğŸ¥ Running health check on pod: $POD"

          # Port-forward for health check (RULE 168)
          kubectl --context spoke port-forward "$POD" -n "$NS" 8081:8080 &
          PF_PID=$!
          sleep 3

          # Check health
          if curl -s -o /dev/null -w "%{http_code}" http://localhost:8081/ | grep -q "200"; then
            echo "âœ… Health check passed (HTTP 200)"
          else
            echo "âš ï¸ Health check returned non-200 status"
          fi

          kill $PF_PID 2>/dev/null || true

      - name: Deployment Summary
        run: |
          NS="${{ env.TENANT }}-${{ env.APP_NAME }}-${{ env.ENV }}"
          IMAGE_TAG="${{ needs.push-to-ecr.outputs.image_tag }}"

          echo "## Deployment Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Field | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Application | ${{ env.APP_NAME }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Environment | ${{ env.ENV }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Image Tag | ${IMAGE_TAG} |" >> $GITHUB_STEP_SUMMARY
          echo "| Namespace | ${NS} |" >> $GITHUB_STEP_SUMMARY
          echo "| Cluster | ${{ env.SPOKE_CLUSTER }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Commit | ${GITHUB_SHA::8} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          echo ""
          echo "ğŸ“Š Pods:"
          kubectl --context spoke get pods -n "$NS" -o wide
          echo ""
          echo "ğŸŒ Services:"
          kubectl --context spoke get svc -n "$NS"
          echo ""
          echo "ğŸ”— Ingress:"
          kubectl --context spoke get ingress -n "$NS"

  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  # Stage 11: Trigger Deployment Landscape
  # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  trigger-landscape:
    name: "ğŸ“Š Trigger Deployment Landscape"
    runs-on: ubuntu-latest
    needs: [verify-deployment]
    if: success()
    steps:
      - name: Trigger Landscape Workflow
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GH_PAT || secrets.GITHUB_TOKEN }}
          script: |
            try {
              await github.rest.actions.createWorkflowDispatch({
                owner: context.repo.owner,
                repo: context.repo.repo,
                workflow_id: '90-deployment-landscape.yaml',
                ref: context.ref || 'main',
                inputs: {
                  environment: '${{ env.ENV }}'
                }
              });
              console.log('âœ… Deployment landscape triggered for ${{ env.ENV }}');
            } catch (error) {
              console.log('âš ï¸ Could not trigger landscape workflow:', error.message);
              console.log('This is non-blocking - deployment succeeded');
            }
